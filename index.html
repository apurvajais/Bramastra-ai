<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Bhramastra AI â€“ Your Smart Dost</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Load React and ReactDOM globally from UMD bundles -->
    <script crossorigin src="https://unpkg.com/react@18/umd/react.production.min.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js"></script>
    <!-- Importmap for @google/genai, as per guidelines -->
    <script type="importmap">
    {
      "imports": {
        "@google/genai": "https://aistudiocdn.com/@google/genai@^1.27.0"
      }
    }
    </script>
    <style>
      /* Basic reset/font setting to ensure consistency, if needed */
      body {
        font-family: ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
      }

      /* Message animations */
      @keyframes fade-in-slide-up {
        from {
          opacity: 0;
          transform: translateY(20px);
        }
        to {
          opacity: 1;
          transform: translateY(0);
        }
      }

      .message-enter-animation {
        animation: fade-in-slide-up 0.5s ease-out forwards;
      }
    </style>
  </head>
  <body class="bg-blue-50">
    <div id="root"></div>

    <!-- Embedded Application Script -->
    <script type="module">
      // Import necessary modules
      import { GoogleGenAI, Chat } from "@google/genai";

      // Global React and ReactDOM from UMD bundles (loaded in <head>)
      const React = window.React;
      const ReactDOM = window.ReactDOM;

      // Type Definitions (as comments, as TypeScript is not used directly here)
      /**
       * @typedef {"user" | "model"} Role
       */
      /**
       * @typedef {Object} Message
       * @property {Role} role
       * @property {string} [text]
       * @property {string} [imageUrl]
       * @property {string} [imagePrompt] // Original prompt for image generation
       */
      /**
       * @typedef {"Hinglish" | "English" | "Hindi"} Language
       */

      // Constants
      const SYSTEM_INSTRUCTIONS = {
        Hinglish: `You are Bhramastra AI, a friendly and helpful AI assistant. 
        Your primary language for conversation is Hinglish (a casual mix of Hindi and English). Your personality is that of a 'smart dost' (a smart friend) â€“ you are warm, empathetic, a bit witty, and always encouraging.
        DO:
        - Use Hinglish phrases naturally, like 'Haan yaar', 'Bilkul!', 'Tension mat lo', 'Kya baat hai!'.
        - Keep sentences conversational and easy to understand.
        - Give helpful suggestions and follow-up ideas proactively.
        - Use emojis to add personality where appropriate. ðŸ˜ŠðŸ‘ðŸŽ‰
        - Maintain the context of the conversation.
        - Adapt your tone based on the user's message.
        DO NOT:
        - Sound like a formal, robotic AI.
        - Use overly complex or pure Hindi/English unless the user switches to it.
        - Forget that you are an AI.
        Your goal is to make the user feel like they are chatting with a knowledgeable and caring friend.`,
        English: `You are Bhramastra AI, a friendly and helpful AI assistant.
        Your primary language for conversation is English. Your personality is that of a 'smart friend' â€“ you are warm, empathetic, witty, and always encouraging.
        You provide clear, helpful answers and can engage in a wide range of topics. Use emojis to add a friendly touch.
        Your goal is to be an excellent, supportive, and knowledgeable English-speaking companion.`,
        Hindi: `à¤†à¤ª à¤¬à¥à¤°à¤¹à¥à¤®à¤¾à¤¸à¥à¤¤à¥à¤° à¤à¤†à¤ˆ à¤¹à¥ˆà¤‚, à¤à¤• à¤®à¤¿à¤¤à¥à¤°à¤µà¤¤ à¤”à¤° à¤¸à¤¹à¤¾à¤¯à¤• à¤à¤†à¤ˆ à¤…à¤¸à¤¿à¤¸à¥à¤Ÿà¥‡à¤‚à¤Ÿà¥¤
        à¤†à¤ªà¤•à¥€ à¤¬à¤¾à¤¤à¤šà¥€à¤¤ à¤•à¥€ à¤ªà¥à¤°à¤¾à¤¥à¤®à¤¿à¤• à¤­à¤¾à¤·à¤¾ à¤¹à¤¿à¤‚à¤¦à¥€ à¤¹à¥ˆà¥¤ à¤†à¤ªà¤•à¤¾ à¤µà¥à¤¯à¤•à¥à¤¤à¤¿à¤¤à¥à¤µ à¤à¤• 'à¤¸à¥à¤®à¤¾à¤°à¥à¤Ÿ à¤¦à¥‹à¤¸à¥à¤¤' à¤•à¤¾ à¤¹à¥ˆ - à¤†à¤ª à¤¸à¥à¤¨à¥‡à¤¹à¥€, à¤¸à¤¹à¤¾à¤¨à¥à¤­à¥‚à¤¤à¤¿à¤ªà¥‚à¤°à¥à¤£, à¤®à¤œà¤¾à¤•à¤¿à¤¯à¤¾ à¤”à¤° à¤¹à¤®à¥‡à¤¶à¤¾ à¤‰à¤¤à¥à¤¸à¤¾à¤¹à¤œà¤¨à¤• à¤¹à¥ˆà¤‚à¥¤
        à¤†à¤ª à¤¸à¥à¤µà¤¾à¤­à¤¾à¤µà¤¿à¤• à¤°à¥‚à¤ª à¤¸à¥‡ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤µà¤¾à¤•à¥à¤¯à¤¾à¤‚à¤¶à¥‹à¤‚ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤œà¥ˆà¤¸à¥‡ 'à¤¹à¤¾à¤ à¤¯à¤¾à¤°', 'à¤¬à¤¿à¤²à¥à¤•à¥à¤²!', 'à¤Ÿà¥‡à¤‚à¤¶à¤¨ à¤®à¤¤ à¤²à¥‹', 'à¤•à¥à¤¯à¤¾ à¤¬à¤¾à¤¤ à¤¹à¥ˆ!'à¥¤
        à¤†à¤ª à¤¬à¤¾à¤¤à¤šà¥€à¤¤ à¤•à¥‡ à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤•à¥‹ à¤¬à¤¨à¤¾à¤ à¤°à¤–à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤”à¤° à¤‰à¤ªà¤¯à¥‹à¤—à¤•à¤°à¥à¤¤à¤¾ à¤•à¥‡ à¤¸à¤‚à¤¦à¥‡à¤¶ à¤•à¥‡ à¤†à¤§à¤¾à¤° à¤ªà¤° à¤…à¤ªà¤¨à¤¾ à¤²à¤¹à¤œà¤¾ à¤…à¤ªà¤¨à¤¾à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤
        à¤†à¤ªà¤•à¤¾ à¤²à¤•à¥à¤·à¥à¤¯ à¤‰à¤ªà¤¯à¥‹à¤—à¤•à¤°à¥à¤¤à¤¾ à¤•à¥‹ à¤¯à¤¹ à¤®à¤¹à¤¸à¥‚à¤¸ à¤•à¤°à¤¾à¤¨à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤µà¥‡ à¤à¤• à¤œà¥à¤žà¤¾à¤¨à¥€ à¤”à¤° à¤¦à¥‡à¤–à¤­à¤¾à¤² à¤•à¤°à¤¨à¥‡ à¤µà¤¾à¤²à¥‡ à¤¦à¥‹à¤¸à¥à¤¤ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤šà¥ˆà¤Ÿ à¤•à¤° à¤°à¤¹à¥‡ à¤¹à¥ˆà¤‚à¥¤`,
      };

      // services/geminiService.ts (adapted for plain JS)
      let aiInstance = null; // Renamed to avoid collision with imported 'ai'

      const getAI = () => {
          if (!aiInstance) {
              // Assume process.env.API_KEY is provided by the environment as per guidelines
              if (typeof process === 'undefined' || !process.env || !process.env.API_KEY) {
                  console.error("API_KEY environment variable is not set. Please ensure it's configured in your environment.");
                  // As per guidelines, must use process.env.API_KEY
                  throw new Error("API_KEY environment variable is not set or accessible. Cannot initialize Gemini AI.");
              }
              aiInstance = new GoogleGenAI({ apiKey: process.env.API_KEY });
          }
          return aiInstance;
      };

      const createChatSession = (systemInstruction) => {
          const genAI = getAI();
          return genAI.chats.create({
              model: 'gemini-2.5-flash',
              config: {
                  systemInstruction,
              },
          });
      };

      const sendMessageToAI = async (chat, message) => {
          try {
              const response = await chat.sendMessage({ message });
              return response.text;
          } catch (error) {
              console.error("Error sending message to Gemini:", error);
              return "Oops! Kuch gadbad ho gayi. Please try again later.";
          }
      };

      const generateImage = async (prompt) => {
          try {
              const genAI = getAI(); // Use the existing memoized instance, as Imagen isn't explicitly listed for API key selection
              const response = await genAI.models.generateImages({
                  model: 'imagen-4.0-generate-001', // High-quality image generation model
                  prompt: prompt,
                  config: {
                      numberOfImages: 1,
                      outputMimeType: 'image/jpeg',
                      aspectRatio: '1:1', // Default aspect ratio
                  },
              });
              const base64ImageBytes = response.generatedImages[0].image.imageBytes;
              return `data:image/jpeg;base64,${base64ImageBytes}`;
          } catch (error) {
              console.error("Error generating image:", error);
              throw new Error("Failed to generate image. Please try again.");
          }
      };

      // hooks/useSpeechRecognition.ts (adapted for plain JS)
      // FIX: Access SpeechRecognition directly from window, as it's a global browser API.
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

      const useSpeechRecognition = (onTranscriptReady) => {
        const [isListening, setIsListening] = React.useState(false);
        const recognitionRef = React.useRef(null);

        React.useEffect(() => {
          if (!SpeechRecognition) {
            console.warn('Speech Recognition is not supported in this browser.');
            return;
          }

          const recognition = new SpeechRecognition();
          recognition.continuous = false;
          recognition.lang = 'hi-IN'; // Set to Hindi for better Hinglish recognition
          recognition.interimResults = false;

          recognition.onstart = () => {
            setIsListening(true);
          };

          recognition.onend = () => {
            setIsListening(false);
          };

          recognition.onerror = (event) => {
            console.error('Speech recognition error:', event.error);
            setIsListening(false);
          };

          recognition.onresult = (event) => {
            const transcript = event.results[0][0].transcript;
            onTranscriptReady(transcript);
          };

          recognitionRef.current = recognition;
        }, [onTranscriptReady]);

        const startListening = () => {
          if (recognitionRef.current && !isListening) {
            recognitionRef.current.start();
          }
        };

        const stopListening = () => {
          if (recognitionRef.current && isListening) {
            recognitionRef.current.stop();
          }
        };

        return { isListening, startListening, stopListening };
      };

      // components/icons.tsx (adapted to React.createElement)
      const MicIcon = ({ className }) => (
          React.createElement("svg", { xmlns: "http://www.w3.org/2000/svg", className: className, viewBox: "0 0 24 24", fill: "currentColor" },
              React.createElement("path", { d: "M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z" }),
              React.createElement("path", { d: "M17 11h-1c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92z" })
          )
      );

      const SendIcon = ({ className }) => (
          React.createElement("svg", { xmlns: "http://www.w3.org/2000/svg", className: className, viewBox: "0 0 24 24", fill: "currentColor" },
              React.createElement("path", { d: "M2.01 21L23 12 2.01 3 2 10l15 2-15 2z" })
          )
      );

      const SpeakerOnIcon = ({ className }) => (
          React.createElement("svg", { xmlns: "http://www.w3.org/2000/svg", className: className, viewBox: "0 0 24 24", fill: "currentColor" },
              React.createElement("path", { d: "M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z" })
          )
      );

      const SpeakerOffIcon = ({ className }) => (
          React.createElement("svg", { xmlns: "http://www.w3.org/2000/svg", className: className, viewBox: "0 0 24 24", fill: "currentColor" },
              React.createElement("path", { d: "M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z" })
          )
      );

      const RobotIcon = ({ className }) => (
          React.createElement("svg", { xmlns: "http://www.w3.org/2000/svg", className: className, viewBox: "0 0 24 24", fill: "currentColor" },
              React.createElement("path", { d: "M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zM8.5 14c-.83 0-1.5-.67-1.5-1.5S7.67 11 8.5 11s1.5.67 1.5 1.5S9.33 14 8.5 14zm3.5-5c-.83 0-1.5-.67-1.5-1.5S11.17 6 12 6s1.5.67 1.5 1.5S12.83 9 12 9zm3.5 5c-.83 0-1.5-.67-1.5-1.5S14.67 11 15.5 11s1.5.67 1.5 1.5S16.33 14 15.5 14z" })
          )
      );

      // components/Header.tsx (adapted to React.createElement)
      const Header = ({ currentLanguage, onLanguageChange, isAudioOutputEnabled, onToggleAudioOutput }) => {
          const languages = ["Hinglish", "English", "Hindi"];

          return (
              React.createElement("header", { className: "bg-white shadow-md w-full z-10" },
                  React.createElement("div", { className: "p-4 container mx-auto flex justify-between items-center" },
                      React.createElement("div", { className: "text-left" },
                          React.createElement("h1", { className: "text-xl md:text-2xl font-bold text-blue-800" }, "Bhramastra AI"),
                          React.createElement("p", { className: "text-xs md:text-sm text-gray-500" }, "Your Smart Dost ðŸ¤–")
                      ),
                      React.createElement("div", { className: "flex items-center space-x-2 md:space-x-4" },
                          React.createElement("div", { className: "flex bg-blue-50 rounded-full p-1" },
                              languages.map((lang) =>
                                  React.createElement("button", {
                                      key: lang,
                                      onClick: () => onLanguageChange(lang),
                                      className: `px-2 py-1 text-xs md:px-3 md:text-sm font-semibold rounded-full transition-colors duration-300 ${
                                          currentLanguage === lang
                                              ? 'bg-blue-700 text-white shadow'
                                              : 'text-blue-700 hover:bg-blue-200'
                                      }`
                                  }, lang)
                              )
                          ),
                          React.createElement("button", {
                              onClick: onToggleAudioOutput,
                              className: "p-2 rounded-full text-blue-700 hover:bg-blue-100 transition-colors duration-300",
                              "aria-label": isAudioOutputEnabled ? "Disable audio replies" : "Enable audio replies"
                          }, isAudioOutputEnabled ? React.createElement(SpeakerOnIcon, { className: "w-6 h-6" }) : React.createElement(SpeakerOffIcon, { className: "w-6 h-6" }))
                      )
                  ),
                  React.createElement("div", { className: "bg-blue-800 text-white text-center text-xs py-1" },
                    "Made by Apoorva Jaiswal"
                  )
              )
          );
      };

      // components/TypingIndicator.tsx (adapted to React.createElement)
      const TypingIndicator = () => {
        return (
          React.createElement("div", { className: "flex items-center space-x-2 p-3" },
            React.createElement("div", { className: "w-2 h-2 bg-blue-500 rounded-full animate-bounce [animation-delay:-0.3s]" }),
            React.createElement("div", { className: "w-2 h-2 bg-blue-500 rounded-full animate-bounce [animation-delay:-0.15s]" }),
            React.createElement("div", { className: "w-2 h-2 bg-blue-500 rounded-full animate-bounce" }),
            React.createElement("span", { className: "text-sm text-gray-500" }, "Bhramastra AI is typing...")
          )
        );
      };

      // components/MessageBubble.tsx (adapted to React.createElement)
      const MessageBubble = ({ message }) => {
        const isUser = message.role === 'user';

        return (
          React.createElement("div", { className: `flex items-start gap-3 my-4 ${isUser ? 'justify-end' : 'justify-start'} message-enter-animation` },
            !isUser && (
              React.createElement("div", { className: "w-8 h-8 rounded-full bg-blue-700 flex items-center justify-center text-white flex-shrink-0" },
                React.createElement(RobotIcon, { className: "w-5 h-5" })
              )
            ),
            React.createElement("div", {
              className: `max-w-xs md:max-w-md lg:max-w-2xl px-4 py-3 rounded-2xl shadow ${
                isUser
                  ? 'bg-blue-700 text-white rounded-br-none'
                  : 'bg-white text-gray-800 rounded-bl-none border border-gray-200'
              }`
            },
              message.text && React.createElement("p", { className: "whitespace-pre-wrap" }, message.text),
              message.imagePrompt && React.createElement("p", { className: "text-xs italic text-gray-400 mb-2" }, `Prompt: "${message.imagePrompt}"`),
              message.imageUrl && React.createElement("img", {
                  src: message.imageUrl,
                  alt: message.imagePrompt || 'Generated image',
                  className: 'mt-2 rounded-lg max-w-full h-auto',
                  loading: 'lazy' // Added lazy loading for images
              })
            )
          )
        );
      };

      // components/ChatWindow.tsx (adapted to React.createElement)
      const ChatWindow = ({ messages, isLoading }) => {
        const chatEndRef = React.useRef(null);

        React.useEffect(() => {
          chatEndRef.current?.scrollIntoView({ behavior: 'smooth' });
        }, [messages, isLoading]);
        
        const welcomeMessage = "Namaste! Main hoon Bhramastra AI. How can I help you today, dost?";

        return (
          React.createElement("div", { className: "flex-1 overflow-y-auto p-4 md:p-6" },
             React.createElement("div", { className: "max-w-4xl mx-auto" },
              messages.length === 0 && !isLoading && (
                  React.createElement("div", { className: "flex flex-col items-center justify-center h-full text-center text-gray-500" },
                      React.createElement("h2", { className: "text-3xl font-bold text-blue-800" }, "Bhramastra AI"),
                      React.createElement("p", { className: "mt-2 text-lg" }, "â€œSmart baat, human touch â€“ Bhramastra AI ke saath!â€"),
                      React.createElement("div", { className: "mt-8" },
                           React.createElement(MessageBubble, { message: {role: 'model', text: welcomeMessage} })
                      )
                  )
              ),
              messages.map((msg, index) =>
                React.createElement(MessageBubble, { key: index, message: msg })
              ),
              isLoading && React.createElement(TypingIndicator, null),
              React.createElement("div", { ref: chatEndRef })
            )
          )
        );
      };

      // components/InputBar.tsx (adapted to React.createElement)
      const InputBar = ({ value, onChange, onSend, onMicClick, onGenerateImage, isListening, disabled }) => {
        return (
          React.createElement("div", { className: "bg-white/80 backdrop-blur-sm border-t border-gray-200 p-4 sticky bottom-0" },
              React.createElement("div", { className: "max-w-4xl mx-auto" },
                  React.createElement("form", { onSubmit: onSend, className: "flex items-center space-x-2 md:space-x-4 bg-gray-100 border border-gray-300 rounded-full p-2 focus-within:ring-2 focus-within:ring-blue-500" },
                      React.createElement("input", {
                          type: "text",
                          value: value,
                          onChange: onChange,
                          placeholder: isListening ? "Listening..." : "Type a message or image prompt...",
                          className: "flex-1 bg-transparent border-none focus:ring-0 text-gray-800 placeholder-gray-500 px-4",
                          disabled: disabled || isListening
                      }),
                      React.createElement("button", {
                          type: "button",
                          onClick: onMicClick,
                          disabled: disabled,
                          className: `p-2 rounded-full transition-colors duration-200 ${
                              isListening 
                                  ? 'bg-red-500 text-white animate-pulse' 
                                  : 'text-blue-700 hover:bg-blue-100'
                          }`,
                          "aria-label": "Use voice input"
                      }, React.createElement(MicIcon, { className: "w-6 h-6" })),
                      React.createElement("button", { // New button for image generation
                          type: "button",
                          onClick: onGenerateImage,
                          disabled: disabled || !value.trim(),
                          className: "bg-purple-700 text-white p-2 text-sm rounded-full hover:bg-purple-800 disabled:bg-purple-300 disabled:cursor-not-allowed transition-colors duration-200 px-4",
                          "aria-label": "Generate image"
                      }, "Image"),
                      React.createElement("button", {
                          type: "submit",
                          disabled: disabled || !value.trim(),
                          className: "bg-blue-700 text-white p-2 rounded-full hover:bg-blue-800 disabled:bg-blue-300 disabled:cursor-not-allowed transition-colors duration-200",
                          "aria-label": "Send message"
                      }, React.createElement(SendIcon, { className: "w-6 h-6" }))
                  )
              )
          )
        );
      };

      // App.tsx content (adapted to React.createElement)
      const App = () => {
          const [messages, setMessages] = React.useState([]);
          const [input, setInput] = React.useState('');
          const [isLoading, setIsLoading] = React.useState(false);
          const [currentLanguage, setCurrentLanguage] = React.useState('Hinglish');
          const [isAudioOutputEnabled, setIsAudioOutputEnabled] = React.useState(false);
          // Added state for image generation
          const [isGeneratingImage, setIsGeneratingImage] = React.useState(false);

          const chatRef = React.useRef(null);
          const synthRef = React.useRef(
              typeof window !== 'undefined' ? window.speechSynthesis : null
          );

          const handleTranscriptReady = React.useCallback((transcript) => {
              setInput(transcript);
          }, []);

          const { isListening, startListening } = useSpeechRecognition(handleTranscriptReady);

          React.useEffect(() => {
              setIsLoading(true);
              chatRef.current = createChatSession(SYSTEM_INSTRUCTIONS[currentLanguage]);
              setMessages([]); // Clear messages on language change
              setIsLoading(false);
          }, [currentLanguage]);
          
          const speak = React.useCallback((text) => {
              if (!isAudioOutputEnabled || !synthRef.current) return;
              
              synthRef.current.cancel();
              const utterance = new SpeechSynthesisUtterance(text);

              const voices = synthRef.current.getVoices();
              let selectedVoice = voices.find(voice => voice.lang.startsWith('hi'));
              if (!selectedVoice) {
                  selectedVoice = voices.find(voice => voice.lang.startsWith('en') && voice.name.includes('Google'));
              }
              if (selectedVoice) {
                  utterance.voice = selectedVoice;
              }
              
              utterance.rate = 1;
              utterance.pitch = 1;
              synthRef.current.speak(utterance);
          }, [isAudioOutputEnabled]);


          const handleSend = async (e) => {
              if (e) e.preventDefault();
              if (!input.trim() || !chatRef.current || isGeneratingImage) return; // Prevent text send during image gen

              const userMessage = { role: 'user', text: input };
              setMessages(prev => [...prev, userMessage]);
              setInput('');
              setIsLoading(true);

              try {
                  const responseText = await sendMessageToAI(chatRef.current, input);
                  const modelMessage = { role: 'model', text: responseText };
                  setMessages(prev => [...prev, modelMessage]);
                  speak(responseText);
              } catch (error) {
                  console.error(error);
                  const errorMessage = { role: 'model', text: 'Sorry, kuch gadbad ho gayi.' };
                  setMessages(prev => [...prev, errorMessage]);
              } finally {
                  setIsLoading(false);
              }
          };

          const handleGenerateImage = async () => {
              if (!input.trim() || isLoading || isGeneratingImage) return;

              const userImagePromptMessage = { role: 'user', text: `Generate image: "${input}"` };
              setMessages(prev => [...prev, userImagePromptMessage]);
              
              const imagePromptText = input; // Save prompt for model message
              setInput('');
              setIsLoading(true);
              setIsGeneratingImage(true);

              try {
                  const imageUrl = await generateImage(imagePromptText);
                  const modelImageMessage = { role: 'model', imageUrl: imageUrl, imagePrompt: imagePromptText };
                  setMessages(prev => [...prev, modelImageMessage]);
                  speak("Here's your image!"); // Audio feedback for image generation
              } catch (error) {
                  console.error(error);
                  const errorMessage = { role: 'model', text: 'Image generation failed. Please try a different prompt.' };
                  setMessages(prev => [...prev, errorMessage]);
              } finally {
                  setIsLoading(false);
                  setIsGeneratingImage(false);
              }
          };
          
          const handleMicClick = () => {
              if (isListening) {
                 // The hook stops automatically on silence or timeout.
                 // A manual stop could be added here if a button press should immediately stop it.
              } else {
                  startListening();
              }
          };
          
          React.useEffect(() => {
              // This effect is for automatically sending voice input
              if (!isListening && input && !isLoading && !isGeneratingImage) {
                  // Check if the last message was the one just transcribed
                  const lastMessage = messages[messages.length-1];
                  const isLastMessageUserTranscription = lastMessage?.role === 'user' && lastMessage?.text === input;

                  // If the current input is a fresh transcription and not already sent
                  if (!isLastMessageUserTranscription) {
                      handleSend();
                  }
              }
              // eslint-disable-next-line react-hooks/exhaustive-deps
          }, [isListening, input, isLoading, isGeneratingImage]); // Added isGeneratingImage to deps


          return (
              React.createElement("div", { className: "flex flex-col h-screen bg-blue-50 font-sans" },
                  React.createElement(Header, {
                      currentLanguage: currentLanguage,
                      onLanguageChange: setCurrentLanguage,
                      isAudioOutputEnabled: isAudioOutputEnabled,
                      onToggleAudioOutput: () => setIsAudioOutputEnabled(prev => !prev)
                  }),
                  React.createElement(ChatWindow, { messages: messages, isLoading: isLoading || isGeneratingImage }), // Pass loading state correctly
                  React.createElement(InputBar, {
                      value: input,
                      onChange: (e) => setInput(e.target.value),
                      onSend: handleSend,
                      onMicClick: handleMicClick,
                      onGenerateImage: handleGenerateImage, // Pass new handler
                      isListening: isListening,
                      disabled: isLoading || isGeneratingImage // Disable input during any AI processing
                  })
              )
          );
      };

      const rootElement = document.getElementById('root');
      if (!rootElement) {
        throw new Error("Could not find root element to mount to");
      }

      const root = ReactDOM.createRoot(rootElement);
      root.render(
        React.createElement(React.StrictMode, null,
          React.createElement(App, null)
        )
      );
    </script>
  </body>
</html>